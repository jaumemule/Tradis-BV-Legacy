{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm\n",
    "import continuous_functions as cf\n",
    "import functions\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "random.seed(1102) # ensuring reproducibility\n",
    "np.random.seed(1102) # same\n",
    "set_random_seed(1102) # yes, python sucks\n",
    "\n",
    "#epochs = int(sys.argv[1]) # input the epochs from command line (for model testing purposes only)\n",
    "epochs = 2 # input the epochs from command line (for model testing purposes only)\n",
    "# using unix or windows, change to False if using Windows.\n",
    "unix = False\n",
    "\n",
    "#  specify the model you want to run:\n",
    "#  options are: 1, 2, \"2reg\", 3, 4, \"chinese\"\n",
    "\n",
    "which_model = \"2reg\"\n",
    "\n",
    "# parameters\n",
    "# these parameters need to be adjusted to optimize the model\n",
    "seq_len = 300            # this is the amount of data (in minutes) used for the forecast\n",
    "forecast_len = 60        # this is the forecast time in minutes\n",
    "dense_units = 16        # this number needs to be played with (last best = 128)\n",
    "lstm_units = 16      # ídem (last best = 64)\n",
    "dropout = 0.75           # also (last best = 0.2)\n",
    "learning_rate = 0.01   # (last best = 0.0001)\n",
    "batch_size = 256*32        #\n",
    "interval = 1            # explanation in following text:\n",
    "\n",
    "\"\"\"\n",
    "The way training data is generated is the following: we take all historical\n",
    "data and store it in df. This is then a dataframe with dimensions\n",
    "number_of_coins * 2 x minutes (the times 2 comes from storing price and volume)\n",
    "For each trading index that we compute we add one number of coins, so, if we\n",
    "are using two indices, we have:\n",
    "\n",
    "                [number_of_coins * 4 x minutes]\n",
    "\n",
    "We then pick \"training_number\" of minutes and use it for prediction, so that df\n",
    "has number_of_coins * 4 x training_number shape. From this, we create training\n",
    "data the following way: we pick every \"interval\" number of minutes and create\n",
    "a training observation of shape number_of_coins * 4 x seq_len together with a target\n",
    "observation of shape number_of_coins * 4 x 1 consisting of the observation\n",
    "\"forecast_len\" away from the last minute of the training data. We store all\n",
    "these training data in a numpy array (X_test) with shape:\n",
    "\n",
    "        [minutes/interval x seq_len x number_of_coins * 4]\n",
    "\n",
    "together with the target data with shape:\n",
    "\n",
    "        [minutes/interval x number_of_coins * 4 x 1]\n",
    "\n",
    "Test data is treated the same but with interval = 1.\n",
    "\n",
    "Note: minutes/interval sizes are not exact because we have to account for\n",
    "data for which we don't have enough historical and stuff like that, so it has\n",
    "minor corrections in the functions.\n",
    "\"\"\"\n",
    "# trading indices\n",
    "\n",
    "# rsi\n",
    "rsi_length = 14     # in days\n",
    "rsi_gap = 24         # in hours\n",
    "\n",
    "# stoch\n",
    "stoch_length = 14   # in days\n",
    "\n",
    "# other parameters\n",
    "\n",
    "train_days = 18         # either this one or the one below will be used\n",
    "pred_days = 3\n",
    "\n",
    "amount = 100      # initial trading amount\n",
    "fee = 0.0014 * 2   # simulated fee from trader\n",
    "p1 = 0.4    # This controls how much money is invested in each prediction,\n",
    "p2 = 0.3    # so the investment on coin 1 will be amount * p1\n",
    "p3 = 0.2    # It's technically not required but obviously they should add up to one\n",
    "p4 = 0.1    # you can add as many percentages as you want, just make sure to add them\n",
    "p5 = 0.     # also in the dictionary below\n",
    "\n",
    "      ###################################\n",
    "###### Do not touch beyond this point!!! ######\n",
    "      ###################################\n",
    "\n",
    "# Packing parameters into dictionaries to make funcions more legible:\n",
    "\n",
    "percentages = {\"p1\" : p1,\n",
    "               \"p2\" : p2,\n",
    "               \"p3\" : p3,\n",
    "               \"p4\" : p4,\n",
    "               \"p5\" : p5}\n",
    "\n",
    "\n",
    "parameters = {\"seq_len\": seq_len,\n",
    "              \"forecast_len\": forecast_len,\n",
    "              \"dense_units\": dense_units,\n",
    "              \"lstm_units\": lstm_units,\n",
    "              \"dropout\": dropout,\n",
    "              \"learning_rate\": learning_rate,\n",
    "              \"batch_size\": batch_size,\n",
    "              \"pred_days\": pred_days,\n",
    "              \"train_days\": train_days,\n",
    "              \"amount\": amount,\n",
    "              \"fee\": fee,\n",
    "              \"interval\": interval,\n",
    "              \"unix\": unix}\n",
    "\n",
    "lrate = LearningRateScheduler(lambda x:  learning_rate / (1. + 0), verbose = 1)\n",
    "# this function lowers the learning rate at each epoch, it's part of the\n",
    "# keras callbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lstm' from 'C:\\\\Users\\\\eudald\\\\Desktop\\\\Crypto-master\\\\Crypto\\\\src\\\\AI\\\\lstm.py'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(functions)\n",
    "importlib.reload(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training model on 120 coins during 57.9 days...\n"
     ]
    }
   ],
   "source": [
    "pre_df = functions.data_load(\"new_data_btc.csv\")\n",
    "    # range included in case you don't want to load the whole file (for testing\n",
    "    # and memory saving purposes)\n",
    "df = functions.filter_coins(pre_df)\n",
    "number_of_coins = int(df.shape[0]/2)\n",
    "training_number = df.shape[1] - 60 * 24 * pred_days # We predict only the last n days\n",
    "\n",
    "parameters[\"number\"] = training_number # add it to the dictionary\n",
    "\n",
    "print(\"> Training model on \" + str(int(df.shape[0]/2)) + \" coins during \" + str(round(training_number/60/24, 1)) + \" days...\")\n",
    "\n",
    "    # This function reshapes the input dataframe into usable training data\n",
    "    # There are two options, train data as it is or also shuffle it.\n",
    "    # TO-DO: check which is best.\n",
    "    #X_train, y_train, scaler = cf.train(df, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsi = functions.compute_rsi(df.iloc[::2])\n",
    "ema = functions.compute_ema(df.iloc[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "period =7\n",
    "leftover = 60*24*period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.iloc[:,leftover:training_number]\n",
    "new_rsi = rsi.iloc[:,leftover:training_number]\n",
    "new_ema = ema.iloc[:,leftover:training_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, scaler = cf.train(new_df, parameters)\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rsi = new_rsi.iloc[:,:training_number - seq_len - forecast_len-leftover:interval].transpose()\n",
    "new_ema = new_ema.iloc[:,:training_number - seq_len - forecast_len-leftover:interval].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.concat([new_rsi, new_ema], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_scaler = StandardScaler().fit(indices)  # define the scaler\n",
    "norm_indices = indices_scaler.transform(indices)  # rescale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_indices = pd.DataFrame(norm_indices, index = indices.index, columns = indices.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [X_train.shape[2], seq_len, dense_units, lstm_units]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Compilation Time :  0.020089387893676758\n"
     ]
    }
   ],
   "source": [
    "model = lstm.build_model2_reg_inputed(dims, dropout, learning_rate, norm_indices.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10948 samples, validate on 3650 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 12s 1ms/step - loss: 1.3429 - val_loss: 469261.8438\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 11s 1ms/step - loss: 98.5471 - val_loss: 469586.8125\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 13s 1ms/step - loss: 63.9873 - val_loss: 469148.0625\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 13s 1ms/step - loss: 12.9471 - val_loss: 469293.0312\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 11s 979us/step - loss: 10.8573 - val_loss: 469355.8125\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 12s 1ms/step - loss: 4.4488 - val_loss: 469401.4375\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 13s 1ms/step - loss: 3.2717 - val_loss: 469422.5000\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 13s 1ms/step - loss: 2.6608 - val_loss: 469418.6562\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 12s 1ms/step - loss: 1.8409 - val_loss: 469403.7500\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "10948/10948 [==============================] - 13s 1ms/step - loss: 1.6124 - val_loss: 469376.2812\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, norm_indices],\n",
    "    y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.25,\n",
    "    callbacks = [lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
